# üîß Ollama Troubleshooting - HallyuHub

Guia completo para diagnosticar e resolver problemas com Ollama.

---

## üö® Problema Comum: Timeouts de 5 Minutos

### Sintomas

- Gera√ß√£o de conte√∫do trava e n√£o completa
- Logs mostram m√∫ltiplas execu√ß√µes sem "‚úÖ Gera√ß√£o conclu√≠da"
- Logs do Ollama mostram `500 | 4m59s` ou `500 | 5m0s`
- `ollama ps` mostra modelo com status "Stopping..."
- Failures altos no relat√≥rio (ex: "Failures: 10")

### Diagn√≥stico

```bash
# 1. Verificar logs do Ollama
ssh root@31.97.255.107 "docker logs hallyuhub-ollama-production --tail 50"

# Procurar por linhas como:
# [GIN] 2026/02/10 - 01:06:39 | 500 | 4m59s | POST "/api/generate"

# 2. Verificar status do modelo
ssh root@31.97.255.107 "docker exec hallyuhub-ollama-production ollama ps"

# Se mostrar "Stopping..." ‚Üí Problema confirmado
```

### Causa Raiz

**Modelo muito pesado para CPU:**
- `phi3:mini` (3.7GB carregado) √© lento demais em CPU
- Sem GPU, processamento leva >5 minutos
- Timeout da aplica√ß√£o: 5 minutos
- Resultado: Erro 500 e falha na gera√ß√£o

### Solu√ß√£o

**Trocar para modelo mais leve (gemma:2b):**

```bash
# 1. Instalar gemma:2b
ssh root@31.97.255.107
cd /var/www/hallyuhub
./scripts/install-gemma-production.sh

# 2. Atualizar .env.production (via Git, n√£o SSH!)
# Adicionar linha:
# OLLAMA_MODEL="gemma:2b"

# 3. Deploy via Git (N√ÉO reiniciar via SSH!)
# Local:
git add .env.production.example
git commit -m "fix(ollama): trocar para gemma:2b (mais leve e r√°pido)"
git push origin staging
# Criar PR para main ‚Üí Deploy autom√°tico
```

---

## üìä Diagn√≥stico Completo

### 1. Verificar se Ollama est√° rodando

```bash
ssh root@31.97.255.107 "docker ps | grep ollama"

# Deve mostrar:
# hallyuhub-ollama-production   Up X hours   (healthy)
```

### 2. Verificar modelos instalados

```bash
ssh root@31.97.255.107 "docker exec hallyuhub-ollama-production ollama list"

# Deve mostrar:
# NAME           ID              SIZE      MODIFIED
# gemma:2b       ...             1.7 GB    ...
```

### 3. Verificar modelo em execu√ß√£o

```bash
ssh root@31.97.255.107 "docker exec hallyuhub-ollama-production ollama ps"

# STATUS deve ser diferente de "Stopping..."
# SIZE deve ser compat√≠vel (gemma:2b ~2-3GB, phi3 ~3-4GB)
```

### 4. Verificar logs de gera√ß√£o

```bash
ssh root@31.97.255.107 "tail -100 /var/www/hallyuhub/logs/cron-direct.log"

# Deve mostrar:
# ‚úÖ Gera√ß√£o conclu√≠da com sucesso
# (N√£o deve travar em "Executando via Docker container")
```

### 5. Verificar logs detalhados

```bash
ssh root@31.97.255.107 "tail -200 /var/www/hallyuhub/logs/auto-generate-2026-02.log | grep -i 'fail\|error\|success'"

# Deve mostrar baixo n√∫mero de Failures:
# Failures: 0-2 (OK)
# Failures: 10+ (PROBLEMA!)
```

---

## üîÑ Compara√ß√£o de Modelos

| Modelo | Tamanho Disco | RAM Usada | Tempo M√©dio | Qualidade | Recomendado |
|--------|---------------|-----------|-------------|-----------|-------------|
| **gemma:2b** | 1.7GB | 2-3GB | 10-30s | Boa | ‚úÖ Production |
| phi3:mini | 2.2GB | 3-4GB | 5+ min | Excelente | ‚ùå Muito lento |
| tinyllama | 637MB | 1-2GB | 5-15s | Regular | ‚úÖ Staging/Testes |

---

## ‚öôÔ∏è Configura√ß√£o de Modelo

### Via Vari√°vel de Ambiente

```bash
# .env.production
OLLAMA_BASE_URL="http://ollama-production:11434"
OLLAMA_MODEL="gemma:2b"  # ‚Üê Definir modelo aqui
```

### Modelos Dispon√≠veis

```typescript
// lib/ai/ai-config.ts
models: {
  default: process.env.OLLAMA_MODEL || 'phi3',
  alternatives: ['mistral', 'llama3:8b', 'tinyllama'],
}
```

---

## üõ†Ô∏è Comandos √öteis

### Gerenciar Modelos

```bash
# Listar modelos instalados
docker exec hallyuhub-ollama-production ollama list

# Baixar novo modelo
docker exec hallyuhub-ollama-production ollama pull gemma:2b

# Remover modelo antigo (liberar espa√ßo)
docker exec hallyuhub-ollama-production ollama rm phi3:mini

# Ver modelo em execu√ß√£o
docker exec hallyuhub-ollama-production ollama ps
```

### Testar Modelo

```bash
# Testar gera√ß√£o (deve responder em < 1 minuto)
docker exec hallyuhub-ollama-production ollama run gemma:2b "Ol√°, como voc√™ est√°?"

# Se demorar >1 min ‚Üí modelo muito pesado
```

### Monitorar Recursos

```bash
# Ver uso de RAM
free -h

# Ver uso de CPU (top 10 processos)
ps aux --sort=-%cpu | head -11

# Ver uso de disco
df -h
```

---

## üöÄ Processo de Troca de Modelo

### Passo a Passo Completo

1. **Identificar problema:**
   ```bash
   # Ver logs do Ollama
   ssh root@31.97.255.107 "docker logs hallyuhub-ollama-production --tail 30"
   ```

2. **Instalar novo modelo:**
   ```bash
   ssh root@31.97.255.107
   cd /var/www/hallyuhub
   ./scripts/install-gemma-production.sh
   ```

3. **Atualizar configura√ß√£o (via Git):**
   ```bash
   # Local
   # Editar .env.production.example
   OLLAMA_MODEL="gemma:2b"

   git add .env.production.example docs/
   git commit -m "fix(ollama): trocar para gemma:2b"
   git push origin staging
   ```

4. **Atualizar .env.production no servidor:**
   ```bash
   # Via GitHub Actions ou manualmente (emerg√™ncia):
   ssh root@31.97.255.107
   cd /var/www/hallyuhub
   nano .env.production
   # Adicionar: OLLAMA_MODEL="gemma:2b"
   ```

5. **Reiniciar aplica√ß√£o (via deploy):**
   ```bash
   # Via Git (recomendado)
   git push origin main  # ‚Üí GitHub Actions faz deploy

   # OU via SSH (emerg√™ncia)
   ssh root@31.97.255.107
   cd /var/www/hallyuhub
   docker-compose -f docker-compose.prod.yml restart hallyuhub
   ```

6. **Verificar funcionamento:**
   ```bash
   # Ver logs em tempo real
   ssh root@31.97.255.107 "tail -f /var/www/hallyuhub/logs/cron-direct.log"

   # Aguardar pr√≥xima execu√ß√£o do cron (a cada 15min)
   # Deve mostrar: ‚úÖ Gera√ß√£o conclu√≠da com sucesso
   ```

---

## üîç Problemas Adicionais

### Ollama n√£o responde

```bash
# Reiniciar Ollama
docker restart hallyuhub-ollama-production

# Verificar sa√∫de
docker ps | grep ollama
# Deve mostrar "(healthy)"
```

### Modelo corrompido

```bash
# Remover e reinstalar
docker exec hallyuhub-ollama-production ollama rm gemma:2b
docker exec hallyuhub-ollama-production ollama pull gemma:2b
```

### Mem√≥ria insuficiente

```bash
# Ver uso de RAM
free -h

# Se <2GB dispon√≠vel ‚Üí problema!
# Solu√ß√£o: Usar modelo menor (tinyllama) ou aumentar RAM
```

---

## üìö Refer√™ncias

- [Ollama Models](https://ollama.com/library)
- [Gemma 2B](https://ollama.com/library/gemma:2b) - Modelo leve e r√°pido
- [Staging Ollama Optimization](./STAGING_OLLAMA_OPTIMIZATION.md)
- [Cron Management](./CRON_MANAGEMENT.md)

---

**√öltima atualiza√ß√£o:** 2026-02-10
**Vers√£o:** 1.0.0
