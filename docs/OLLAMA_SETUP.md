# Configura√ß√£o do Ollama em Docker

O Ollama √© usado como provedor de IA local para gerar conte√∫do quando outros provedores (Gemini, OpenAI, Claude) n√£o est√£o dispon√≠veis ou para reduzir custos.

## üìã Vis√£o Geral

- **Ambiente**: Docker container separado para staging e produ√ß√£o
- **Modelo**: phi3 (~2.2GB, leve e r√°pido)
- **Uso de RAM**: ~2-3GB quando ativo
- **Integra√ß√£o**: Autom√°tica via rede Docker

## üöÄ Setup R√°pido

### 1. Criar Volumes Docker

Primeiro, crie os volumes persistentes para armazenar os modelos:

```bash
# Para staging
docker volume create ollama-staging-data

# Para produ√ß√£o
docker volume create ollama-production-data
```

### 2. Iniciar Containers

```bash
# Staging
docker-compose -f docker-compose.staging.yml up -d

# Produ√ß√£o
docker-compose -f docker-compose.prod.yml up -d
```

### 3. Baixar Modelo phi3

Execute o script automatizado:

```bash
# Staging
./scripts/setup-ollama-docker.sh staging

# Produ√ß√£o
./scripts/setup-ollama-docker.sh production
```

O script vai:
- ‚úÖ Verificar se o container est√° rodando
- ‚úÖ Baixar o modelo phi3 (~2.2GB)
- ‚úÖ Testar a gera√ß√£o de texto
- ‚úÖ Mostrar modelos instalados

### 4. Atualizar Vari√°veis de Ambiente

As vari√°veis j√° est√£o configuradas nos arquivos `.env.staging` e `.env.production`:

```bash
# Staging
OLLAMA_BASE_URL="http://ollama-staging:11434"

# Produ√ß√£o
OLLAMA_BASE_URL="http://ollama-production:11434"
```

### 5. Reiniciar Aplica√ß√£o

Ap√≥s configurar o Ollama, reinicie a aplica√ß√£o para reconhecer o provedor:

```bash
# Staging
docker-compose -f docker-compose.staging.yml restart hallyuhub-staging

# Produ√ß√£o
docker-compose -f docker-compose.prod.yml restart hallyuhub
```

## ‚úÖ Verifica√ß√£o

### Verificar Container

```bash
# Staging
docker ps | grep ollama-staging

# Produ√ß√£o
docker ps | grep ollama-production
```

### Listar Modelos

```bash
# Staging
docker exec hallyuhub-ollama-staging ollama list

# Produ√ß√£o
docker exec hallyuhub-ollama-production ollama list
```

### Testar Gera√ß√£o

```bash
# Staging
docker exec -it hallyuhub-ollama-staging ollama run phi3 "Ol√°, como voc√™ est√°?"

# Produ√ß√£o
docker exec -it hallyuhub-ollama-production ollama run phi3 "Ol√°, como voc√™ est√°?"
```

### Verificar Logs

```bash
# Staging
docker logs hallyuhub-ollama-staging -f

# Produ√ß√£o
docker logs hallyuhub-ollama-production -f
```

### Endpoint Health

Teste o endpoint de health da aplica√ß√£o:

```bash
# Staging
curl http://localhost:3001/api/health

# Produ√ß√£o (via Nginx)
curl https://www.hallyuhub.com.br/api/health
```

Procure por:
```json
{
  "aiProviders": {
    "ollama": {
      "available": true,
      "url": "http://ollama-production:11434"
    }
  }
}
```

## üîß Troubleshooting

### Container n√£o inicia

```bash
# Ver logs
docker logs hallyuhub-ollama-staging --tail 50

# Verificar se porta 11434 est√° livre
docker ps | grep 11434

# Reiniciar container
docker restart hallyuhub-ollama-staging
```

### Modelo n√£o baixa

```bash
# Verificar espa√ßo em disco
df -h

# Baixar manualmente
docker exec hallyuhub-ollama-staging ollama pull phi3

# Ver progresso
docker logs hallyuhub-ollama-staging -f
```

### Aplica√ß√£o n√£o conecta

```bash
# Verificar se est√£o na mesma rede
docker network inspect web

# Testar conectividade dentro do container
docker exec hallyuhub-staging curl http://ollama-staging:11434/api/tags

# Verificar vari√°vel de ambiente
docker exec hallyuhub-staging env | grep OLLAMA
```

### Alto uso de mem√≥ria

```bash
# Ver uso de recursos
docker stats hallyuhub-ollama-production

# Limitar mem√≥ria (adicionar ao docker-compose.yml):
deploy:
  resources:
    limits:
      memory: 4G
```

### Modelo corrompido

```bash
# Remover modelo
docker exec hallyuhub-ollama-production ollama rm phi3

# Baixar novamente
docker exec hallyuhub-ollama-production ollama pull phi3
```

## üìä Monitoramento

### Uso de Recursos

```bash
# Ver stats em tempo real
docker stats hallyuhub-ollama-production

# Ver uso de disco do volume
docker system df -v | grep ollama
```

### Logs de Acesso

Os logs da aplica√ß√£o mostram quando o Ollama √© usado:

```bash
docker logs hallyuhub -f | grep -i ollama
```

## üîÑ Atualiza√ß√£o

### Atualizar Imagem Ollama

```bash
# Pull da nova vers√£o
docker pull ollama/ollama:latest

# Recriar container (mant√©m modelos no volume)
docker-compose -f docker-compose.prod.yml up -d --force-recreate ollama-production
```

### Atualizar Modelo phi3

```bash
docker exec hallyuhub-ollama-production ollama pull phi3
```

## üéØ Modelos Alternativos

Se precisar trocar o modelo (mais RAM dispon√≠vel):

```bash
# Mistral (4GB RAM) - melhor qualidade
docker exec hallyuhub-ollama-production ollama pull mistral

# Llama3:8b (8GB RAM) - qualidade m√°xima
docker exec hallyuhub-ollama-production ollama pull llama3:8b

# Atualizar c√≥digo em lib/ai-orchestration.ts
# Trocar "phi3" por "mistral" ou "llama3:8b"
```

## üóëÔ∏è Remo√ß√£o

Para remover completamente o Ollama:

```bash
# Parar e remover container
docker-compose -f docker-compose.prod.yml stop ollama-production
docker-compose -f docker-compose.prod.yml rm -f ollama-production

# Remover volume (CUIDADO: apaga modelos!)
docker volume rm ollama-production-data

# Remover configura√ß√£o do .env
# Comentar OLLAMA_BASE_URL
```

## üí° Dicas

1. **Primeiro Deploy**: O modelo phi3 s√≥ precisa ser baixado uma vez. Depois fica persistido no volume Docker.

2. **Economia de Custos**: Ollama √© gratuito e roda localmente, ideal como fallback quando APIs pagas atingem limites.

3. **Performance**: phi3 √© r√°pido mas menos capaz que GPT-4/Gemini. Use para tarefas simples ou emergenciais.

4. **Recursos**: Garanta pelo menos 4GB RAM livre no servidor antes de usar Ollama.

5. **Backup**: Os modelos ficam em `/root/.ollama` dentro do container. O volume Docker j√° persiste isso.

## üìö Recursos

- [Ollama Official Docs](https://ollama.com/docs)
- [Ollama Docker Hub](https://hub.docker.com/r/ollama/ollama)
- [Modelos Dispon√≠veis](https://ollama.com/library)
